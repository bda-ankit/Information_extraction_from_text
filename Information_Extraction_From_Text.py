# -*- coding: utf-8 -*-
"""dmw_poster_presentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JP8N3QKvGJUYCB8tF5vk2Ijs0dYzyEqb

INFORMATION EXTRACTION FROM TEXT MESSAGES USING DATA MINING
"""

!pip install nltk

!pip install spacy

import nltk
nltk.download('punkt')

!python -m spacy download en_core_web_sm

text="Text mining, also referred to as text data mining, similar to text analytics, is the process of deriving high-quality information from text. It involves the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources. Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can differ three different perspectives of text mining"

"""# sentence-tokenization"""

sentences=nltk.sent_tokenize(text)

for sentence in sentences: 
  print(sentence)

for sentence in sentences:
  words=nltk.word_tokenize(sentence)
  for word in words:
    print(word)

"""# stemming"""

from nltk.stem import PorterStemmer

ps=PorterStemmer()

for sentence in sentences:
  words=nltk.word_tokenize(sentence)
  for word in words:
    rootWord=ps.stem(word)
    print("Root of", word,"is",rootWord)

"""# lemmetization"""

from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')

wordnet_lemmatizer=WordNetLemmatizer()

for sentence in sentences:
  words=nltk.word_tokenize(sentence)
  for word in words:
    rootWord=wordnet_lemmatizer.lemmatize(word)
    print("lemma of", word,"is",rootWord)

"""# stop word removal"""

nltk.download('stopwords')

from nltk.corpus import stopwords
print(stopwords.words('english'))

stop_words=stopwords.words('english')
text_tokens=nltk.word_tokenize(text)
tokens_without_sw=[word for word in text_tokens if not word in stop_words]
print(tokens_without_sw)

"""# part of speech tagging"""

import spacy
from spacy import displacy
sp=spacy.load('en_core_web_sm')

for sentence in sentences:
  sen=sp(sentence)
  displacy.render(sen, style='dep',jupyter=True ,options={'distance': 85})

"""# name entity recongnition"""

sen=sp(text)
print(sen.ents)

sen=sp(text)
for entity in sen.ents:
  print(entity.text+'-'+entity.label_+'-'+str(spacy.explain(entity.label_)))

for entity in sen.ents:
  if entity.label_=='GPE':
    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))

spacy.displacy.render(sen,style='ent',jupyter=True)